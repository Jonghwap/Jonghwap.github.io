---
title: "The fast rate of convergence of the smooth adapted Wasserstein distance"
year: 2025
journal: Under revision
coauthors:
  - <a href="https://sites.google.com/view/martin-larsson" target="_blank" rel="noopener">Martin Larsson</a>
  - <a href="https://sites.google.com/view/johannes-wiesel" target="_blank" rel="noopener">Johannes Wiesel</a>
arxiv: https://arxiv.org/abs/2503.10827
journal_url:
order: 4
abstract: >-
  Estimating a $d$-dimensional distribution $\mu$ by the empirical measure $\hat{\mu}_n$ of its samples is an important task in probability theory, statistics and machine learning. It is well known that $\mathbb{E}[\mathcal{W}_p(\hat{\mu}_n, \mu)]\lesssim n^{-1/d}$ for $d>2p$, where $\mathcal{W}_p$ denotes the $p$-Wasserstein metric. An effective tool to combat this curse of dimensionality is the smooth Wasserstein distance $\mathcal{W}^{(\sigma)}_p$, which measures the distance between two probability measures after having convolved them with isotropic Gaussian noise $\mathcal{N}(0,\sigma^2\text{I})$. We show that the smooth adapted Wasserstein distance $\mathcal{A}\mathcal{W}_p^{(\sigma)}$ achieves the fast rate of convergence $\mathbb{E}[\mathcal{A}\mathcal{W}_p^{(\sigma)}(\hat{\mu}_n, \mu)]\lesssim n^{-1/2}$, if $\mu$ is subgaussian. This result follows from the surprising fact, that any subgaussian measure $\mu$ convolved with a Gaussian distribution has locally Lipschitz kernels.
---
